import os
from dotenv import load_dotenv
import time
import html
import tweepy
from tweepy.errors import TweepyException, TooManyRequests, HTTPException as TweepyHTTPException
from typing import Optional, List
import requests
import re
# from googletrans import Translator
from openai import OpenAI
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
import random
import psutil
import gc
import feedparser
from email.utils import parsedate_to_datetime
from datetime import timezone, datetime
import json

load_dotenv()
TWITTER_BEARER_TOKEN = os.getenv("TWITTER_BEARER_TOKEN")
TWITTER_USERNAMES = os.getenv("TWITTER_USERNAMES", "").split(",")
TWITTER_USER_IDS = os.getenv("TWITTER_USER_IDS", "").split(",")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_TEST_CHANNEL_ID = os.getenv("TELEGRAM_TEST_CHANNEL_ID")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
NAVER_INVOKE_URL = os.getenv("NAVER_INVOKE_URL")
MS_TRANSLATOR_KEY = os.getenv("MS_TRANSLATOR_KEY")
MS_TRANSLATOR_REGION = os.getenv("MS_TRANSLATOR_REGION")
DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMOJI_PATTERN = re.compile("[\U0001F300-\U0001FAFF\U00002700-\U000027BF]+", flags=re.UNICODE)
URL_PATTERN = re.compile(r"https?://[^\s\)\]\}]+", re.IGNORECASE)
NL_TOKEN = "[[NL]]"

# ì´ˆê¸° ì„¤ì •
client = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN,
    wait_on_rate_limit=True  # 429ì¼ ë•Œ ìë™ ëŒ€ê¸°
)
_gpt_client = OpenAI(api_key=OPENAI_API_KEY)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHECK_INTERVAL_SECONDS = 1000
MAX_CAPTION_LENGTH = 1000  # í…”ë ˆê·¸ë¨ ì•ˆì „ ë²”ìœ„
TEXT_LENGTH_THRESHOLD = 250  # í¬ë¡¤ë§ì„ ì‹œì‘í•  í…ìŠ¤íŠ¸ ê¸¸ì´ ì„ê³„ê°’
LAST_ID_JSON_PATH = os.path.join(BASE_DIR, "x_last_ids.json")
# íŠ¹ì • ìœ ì €ì˜ quoted íŠ¸ìœ—ì€ ì œì™¸í•  ë•Œ ì“°ëŠ” ë¦¬ìŠ¤íŠ¸
EXCLUDE_QUOTE_USERS = [
    "105353526",            # markminervini
    "25073877",             # realDonaldTrump
    "1406461126917849096",  # mmsbml
]

# TruthSocialTrump ì¶”ê°€
RSS_URL = "https://trumpstruth.org/feed"
TRUMP_STATE_FILE = "trump_truth_last_ts.txt"
TRUMP_USERNAME = "TruthSocial_Trump"

def _load_last_ids() -> dict:
    """x_last_ids.jsonì—ì„œ ì „ì²´ ë§¤í•‘ ë¶ˆëŸ¬ì˜¤ê¸°"""
    try:
        with open(LAST_ID_JSON_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data
    except FileNotFoundError:
        return {}
    except Exception as e:
        print(f"âš ï¸ last_ids.json ë¡œë“œ ì˜¤ë¥˜: {e}")
    return {}

def _save_last_ids(data: dict):
    """ì „ì²´ ë§¤í•‘ì„ x_last_ids.jsonì— ì €ì¥"""
    try:
        with open(LAST_ID_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            f.flush()
            os.fsync(f.fileno())
    except Exception as e:
        print(f"âš ï¸ last_ids.json ì €ì¥ ì˜¤ë¥˜: {e}")

def get_last_id(user_id: str) -> Optional[int]:
    """
    x_last_ids.jsonì—ì„œ í•´ë‹¹ user_idì˜ last_idë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    ì—†ìœ¼ë©´ None.
    """
    data = _load_last_ids()
    value = data.get(user_id)
    if value is None:
        return None
    try:
        return int(value)
    except (TypeError, ValueError):
        return None

def save_last_id(user_id: str, tweet_id: int):
    """
    x_last_ids.jsonì— user_id -> tweet_id ë§¤í•‘ì„ ì €ì¥.
    ê¸°ì¡´ ê°’ì€ ë®ì–´ì”€.
    """
    data = _load_last_ids()
    data[user_id] = int(tweet_id)
    _save_last_ids(data)

    path = os.path.join(BASE_DIR, f"last_id_{user_id}.txt")
    with open(path, "w") as f:
        f.write(str(tweet_id))
        f.flush()
        os.fsync(f.fileno())

def mask_newlines(text: str) -> str:
    return text.replace("\r\n", "\n").replace("\n", NL_TOKEN)

def restore_newlines(text: str) -> str:
    import re
    return re.sub(r"\[\[nl\]\]", "\n", text, flags=re.IGNORECASE)

class TwitterCrawler:
    def __init__(self):
        self.setup_driver()
        
    def setup_driver(self):
        chrome_options = Options()
        
        # ê¸°ë³¸ ì„¤ì •
        chrome_options.add_argument("--headless=new")           # ìµœì‹  í—¤ë“œë¦¬ìŠ¤
        chrome_options.add_argument("--disable-gpu")            # GPU ë¹„í™œì„± (ìœˆë„ìš°/ê°€ìƒí™˜ê²½ í•„ìˆ˜)
        chrome_options.add_argument("--use-gl=swiftshader")     # ì†Œí”„íŠ¸ì›¨ì–´ ë Œë”ëŸ¬
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # ë´‡ ê°ì§€ ë°©ì§€ ì„¤ì •
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ë‹¤ì–‘í•œ User-Agent ëœë¤ ì„ íƒ
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0"
        ]
        selected_ua = random.choice(user_agents)
        chrome_options.add_argument(f"--user-agent={selected_ua}")
        
        # ì¶”ê°€ ë´‡ ê°ì§€ ë°©ì§€
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--allow-running-insecure-content")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # JavaScriptë¡œ ë´‡ ê°ì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
        self.driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
        
        self.wait = WebDriverWait(self.driver, 15)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
    

    def crawl_full_tweet_text(self, tweet_id, username):
        try:
            url = f"https://x.com/{username}/status/{tweet_id}"
            print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {url}")
            time.sleep(random.uniform(1.5, 3.5))
            self.driver.get(url)

            # ë³¸ë¬¸ì´ ë³´ì¼ ë•Œê¹Œì§€ ëŒ€ê¸°
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweetText"]')))

            # ì‚¬ëŒì²˜ëŸ¼ ìŠ¤í¬ë¡¤
            self.simulate_human_behavior()

            full_text = self.extract_tweet_text()
            return full_text
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    
    def simulate_human_behavior(self):
        """ì¸ê°„ê³¼ ìœ ì‚¬í•œ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
        try:
            # ëœë¤ ìŠ¤í¬ë¡¤
            scroll_amount = random.randint(100, 300)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
            time.sleep(random.uniform(0.5, 1.5))
            
            # ë‹¤ì‹œ ìœ„ë¡œ ìŠ¤í¬ë¡¤
            self.driver.execute_script(f"window.scrollBy(0, -{scroll_amount//2});")
            time.sleep(random.uniform(0.3, 0.8))
            
            # ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜ (í—¤ë“œë¦¬ìŠ¤ì—ì„œëŠ” íš¨ê³¼ ì—†ì§€ë§Œ íŒ¨í„´ ê°ì§€ ë°©ì§€)
            self.driver.execute_script("""
                var event = new MouseEvent('mousemove', {
                    'view': window,
                    'bubbles': true,
                    'cancelable': true,
                    'clientX': arguments[0],
                    'clientY': arguments[1]
                });
                document.dispatchEvent(event);
            """, random.randint(100, 800), random.randint(100, 600))
            
        except Exception as e:
            print(f"âš ï¸ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            pass

    def _html_to_text_with_emojis(self, html_str: str) -> str:
        # 1) <img ... alt="ğŸ™‚" ...> â†’ ğŸ™‚ ë¡œ ì¹˜í™˜ (Twemoji)
        html_str = re.sub(
            r'<img[^>]*\salt="([^"]+)"[^>]*>',
            lambda m: html.unescape(m.group(1)),
            html_str,
            flags=re.IGNORECASE
        )
        # 2) <svg ... aria-label="ğŸ™‚" ...>...</svg> â†’ ğŸ™‚
        html_str = re.sub(
            r'<svg[^>]*\saria-label="([^"]+)"[^>]*>.*?</svg>',
            lambda m: html.unescape(m.group(1)),
            html_str,
            flags=re.IGNORECASE | re.DOTALL
        )
        # 3) ë§í¬ í…ìŠ¤íŠ¸ ë“± span ì •ë¦¬: íƒœê·¸ ì œê±° ì „, ì¤„ë°”ê¿ˆì€ ë³´ì¡´
        # <br> â†’ ì¤„ë°”ê¿ˆ
        html_str = re.sub(r'<br\s*/?>', '\n', html_str, flags=re.IGNORECASE)

        # 4) ë‚¨ì€ íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', html_str)

        # 5) HTML ì—”í‹°í‹° ë””ì½”ë“œ (&amp; ë“±)
        text = html.unescape(text)

        # 6) ê³µë°± ì •ë¦¬
        return re.sub(r'[ \t\f\v]+', ' ', text).strip()
        
    def extract_tweet_text(self):
        try:
            tweet_selectors = [
                '[data-testid="tweetText"]',
                'article[data-testid="tweet"] span[dir="auto"]',
                'div[data-testid="tweetText"]',
                'span[data-testid="tweetText"]'
            ]
            for selector in tweet_selectors:
                try:
                    el = self.driver.find_element(By.CSS_SELECTOR, selector)
                    # í•µì‹¬: innerHTMLë¡œ ê°€ì ¸ì™€ì„œ ì´ëª¨ì§€ ë³µì›
                    inner_html = el.get_attribute("innerHTML")
                    if inner_html:
                        text = self._html_to_text_with_emojis(inner_html)
                        if text:
                            print(f"âœ… íŠ¸ìœ— í…ìŠ¤íŠ¸(ì´ëª¨ì§€ í¬í•¨) ì¶”ì¶œ ì„±ê³µ: {len(text)}ì")
                            return text
                except NoSuchElementException:
                    continue
            print("âŒ íŠ¸ìœ— í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def close(self):
        if self.driver:
            self.driver.quit()

# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì§€ì—° ì´ˆê¸°í™”)
crawler = None
crawler_created_time = None
CRAWLER_RESTART_INTERVAL = 3600  # 1ì‹œê°„ë§ˆë‹¤ í¬ë¡¤ëŸ¬ ì¬ì‹œì‘

def get_crawler():
    """í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•„ìš”í•  ë•Œë§Œ ìƒì„±í•˜ê³  ì •ê¸°ì ìœ¼ë¡œ ì¬ì‹œì‘"""
    global crawler, crawler_created_time
    
    current_time = time.time()
    
    # í¬ë¡¤ëŸ¬ê°€ ì—†ê±°ë‚˜ ì˜¤ë˜ëœ ê²½ìš° ì¬ìƒì„±
    if (crawler is None or 
        crawler_created_time is None or 
        current_time - crawler_created_time > CRAWLER_RESTART_INTERVAL):
        
        # ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì •ë¦¬
        if crawler:
            try:
                print("ğŸ”„ í¬ë¡¤ëŸ¬ ì¬ì‹œì‘ ì¤‘...")
                crawler.close()
                print("âœ… ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìƒˆ í¬ë¡¤ëŸ¬ ìƒì„±
        print("ğŸ”§ ìƒˆ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        crawler = TwitterCrawler()
        crawler_created_time = current_time
        print("âœ… ìƒˆ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    return crawler

def replace_emojis_with_tags(text):
    emojis = []
    def replacer(match):
        emojis.append(match.group(0))
        return f"[EMOJI_{len(emojis) - 1}]"
    new_text = EMOJI_PATTERN.sub(replacer, text)
    return new_text, emojis

def restore_emojis(translated_text, emojis):
    for idx, emoji in enumerate(emojis):
        pattern = re.compile(rf"\[emoji_{idx}\]", re.IGNORECASE)
        translated_text = pattern.sub(emoji, translated_text)
    return translated_text

def mask_urls(text):
    urls = []
    def replacer(match):
        urls.append(match.group(0))
        return f"[URL_{len(urls) - 1}]"
    new_text = URL_PATTERN.sub(replacer, text)
    return new_text, urls

def restore_urls(text, urls):
    for idx, url in enumerate(urls):
        pattern = re.compile(rf"\[url_{idx}\]", re.IGNORECASE)
        # ì•ë’¤ ê³µë°±ì´ ì—†ìœ¼ë©´ ë„ì–´ì“°ê¸° ê°•ì œ ì‚½ì…
        text = pattern.sub(f" {url} ", text)
    return text

def translate(text):
    for engine in [translate_with_gpt4omini, translate_with_mymemory, translate_with_microsoft, translate_with_deepl]:
        try:
            return engine(text)
        except Exception as e:
            print(f"âš ï¸ {engine.__name__} ì‹¤íŒ¨: {e}")
    # ëª¨ë“  ë²ˆì—­ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    return None

def translate_preserving_emojis_and_urls(original_text):
    # 1. ì´ëª¨ì§€ ë§ˆìŠ¤í‚¹
    emoji_tagged_text, emojis = replace_emojis_with_tags(original_text)
    # 2. URL ë§ˆìŠ¤í‚¹
    url_tagged_text, urls = mask_urls(emoji_tagged_text)
    # 3. ë²ˆì—­
    translated = translate(url_tagged_text)  # ìˆœì°¨ì  ë²ˆì—­ê¸° í˜¸ì¶œ
    # 4. ì´ëª¨ì§€ ë³µì›
    text_with_emoji = restore_emojis(translated, emojis)
    # 5. URL ë³µì›
    fully_restored = restore_urls(text_with_emoji, urls)
    return fully_restored

def translate_with_gpt4omini(text: str, target_lang: str = "ko", source_lang: str | None = None) -> str:
    """
    GPTâ€‘4o minië¡œ ë²ˆì—­.
    - ì´ëª¨ì§€, URL, @ë©˜ì…˜, #í•´ì‹œíƒœê·¸, $í‹°ì»¤, [EMOJI_1] ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”, ì¤„ë°”ê¿ˆ/ê³µë°±ì€ ì›í˜• ë³´ì¡´.
    - translate_preserving_emojis_and_urls()ê°€ ì´ë¯¸ ë§ˆìŠ¤í‚¹/ë³µì›ì„ í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ ë²ˆì—­ë§Œ.
    """
    if not text or text.strip() == "":
        return text

    system_msg = (
        "You are a precise translator. Translate ONLY natural language segments into the target language. "
        "STRICTLY preserve as-is: emojis, URLs (https://...), emails, @mentions, #hashtags, $tickers, "
        "any placeholders like [EMOJI_0], {EMOJI_1}, [[EMOJI_2]] with exact casing and brackets, "
        "code blocks/inline code, and original line breaks/spaces. "
        "Do not add extra text. Output only the translation."
    )

    # source_langì€ ì„ íƒ ì‚¬í•­ (ì§€ì •í•˜ì§€ ì•Šì•„ë„ ë¨)
    if source_lang:
        user_msg = f"Source language: {source_lang}\nTarget language: {target_lang}\n\nText:\n{text}"
    else:
        user_msg = f"Target language: {target_lang}\n\nText:\n{text}"

    resp = _gpt_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
    )
    out = resp.choices[0].message.content or ""
    return out.strip()

def translate_with_mymemory(text, source="en", target="ko"):
    # 480ì ì´ìƒì´ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ë²ˆì—­
    if len(text) > 430:
        print(f"ğŸ“ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(text)}ì), MyMemory ë¶„í•  ë²ˆì—­ ì‹œì‘")
        sentences = split_text_into_sentences(text)
        translated_parts = []
        
        current_part = ""
        
        for sentence in sentences:
            # í˜„ì¬ ë¶€ë¶„ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
            if len(current_part + sentence) <= 430:
                current_part += sentence
            else:
                # í˜„ì¬ ë¶€ë¶„ ë²ˆì—­
                if current_part:
                    translated_part = translate_mymemory_part(current_part, source, target)
                    if translated_part:
                        translated_parts.append(translated_part)
                
                # ìƒˆ ë¶€ë¶„ ì‹œì‘
                current_part = sentence
        
        # ë§ˆì§€ë§‰ ë¶€ë¶„ ë²ˆì—­
        if current_part:
            translated_part = translate_mymemory_part(current_part, source, target)
            if translated_part:
                translated_parts.append(translated_part)
        
        # ë²ˆì—­ëœ ë¶€ë¶„ë“¤ì„ í•©ì¹˜ê¸°
        if translated_parts:
            return "".join(translated_parts)
        else:
            raise Exception("MyMemory ë¶„í•  ë²ˆì—­ ì‹¤íŒ¨")
    
    # 480ì ì´í•˜ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë²ˆì—­
    return translate_mymemory_part(text, source, target)

def translate_mymemory_part(text, source="en", target="ko"):
    """MyMemory APIë¡œ í…ìŠ¤íŠ¸ ë²ˆì—­ (ë‹¨ì¼ ë¶€ë¶„)"""
    url = "https://api.mymemory.translated.net/get"
    params = {
        "q": text,
        "langpair": f"{source}|{target}"
    }

    response = requests.get(url, params=params, timeout=5)
    data = response.json()
    result = data["responseData"]["translatedText"]

    if "MYMEMORY WARNING" in result.upper():
        raise Exception("MyMemory usage limit reached")

    return result

def split_text_into_sentences(text):
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
    # ë¬¸ì¥ êµ¬ë¶„ìë“¤
    sentence_endings = ['.', '!', '?']
    
    sentences = []
    current_sentence = ""
    
    for char in text:
        current_sentence += char
        
        if char in sentence_endings:
            if current_sentence.strip():
                sentences.append(current_sentence.strip())
            current_sentence = ""
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
    if current_sentence.strip():
        sentences.append(current_sentence.strip())
    
    return sentences

# def translate_with_googletrans(text, dest='ko'):
#     translator = Translator()
#     try:
#         result = translator.translate(text, dest=dest)
#         return result.text
#     except Exception as e:
#         print("âŒ googletrans ì˜¤ë¥˜:", e)
#         return "[Google Translate ì‹¤íŒ¨]"

def translate_with_microsoft(text):
    url = "https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&from=en&to=ko"
    headers = {
        "Ocp-Apim-Subscription-Key": os.getenv("MS_TRANSLATOR_KEY"),
        "Ocp-Apim-Subscription-Region": os.getenv("MS_TRANSLATOR_REGION"),
        "Content-type": "application/json"
    }
    body = [{"text": text}]
    response = requests.post(url, headers=headers, json=body)
    if response.status_code == 429:
        raise Exception("Microsoft usage limit exceeded")
    response.raise_for_status()
    return response.json()[0]["translations"][0]["text"]

def translate_with_deepl(text):
    url = "https://api-free.deepl.com/v2/translate"
    headers = {"Authorization": f"DeepL-Auth-Key {DEEPL_API_KEY}"}
    data = {
        "text": text,
        "target_lang": "KO",  # í•œêµ­ì–´
    }
    response = requests.post(url, headers=headers, data=data)
    if response.status_code == 456:
        raise Exception("DeepL usage limit exceeded")
    response.raise_for_status()
    return response.json()["translations"][0]["text"]

def extract_emojis(text):
    emojis = EMOJI_PATTERN.findall(text)
    cleaned = EMOJI_PATTERN.sub("", text)
    return emojis, cleaned.strip()

def merge_emojis_back(emojis, translated_text):
    return f"{''.join(emojis)} {translated_text}"

def get_latest_tweet(user_id, last_id=None, max_results=10):
    return call_with_retry(
        client.get_users_tweets,
        id=user_id,
        since_id=last_id,
        max_results=max_results,
        # âœ… ë‹µê¸€/ë¦¬íŠ¸ìœ— ì œì™¸ â†’ Posts íƒ­ê³¼ ì¼ì¹˜
        exclude=["replies", "retweets"],
        tweet_fields=["created_at", "id", "text", "attachments", "referenced_tweets"],
        expansions=["attachments.media_keys", "referenced_tweets.id"],
        media_fields=["url", "type"]
    )

def iterate_user_tweets(user_id: str, since_id: Optional[int], page_size: int = 10):
    """
    since_id ì´í›„ì˜ ëª¨ë“  íŠ¸ìœ—ì„ 'ì˜¤ë˜ëœ ê²ƒë¶€í„°' yield.
    ê° yieldëŠ” (tweet, includes) íŠœí”Œ.
    - page_size: 10~100 (íŠ¸ìœ„í„° ì œí•œ). 100 ê¶Œì¥.
    """
    next_token = None
    pages = []

    while True:
        resp = call_with_retry(
            client.get_users_tweets,
            id=user_id,
            since_id=since_id,
            max_results=page_size,
            exclude=["replies", "retweets"],  # Posts íƒ­ê³¼ ì¼ì¹˜
            tweet_fields=["created_at", "id", "text", "attachments", "referenced_tweets"],
            expansions=["attachments.media_keys", "referenced_tweets.id"],
            media_fields=["url", "type"],
            pagination_token=next_token
        )

        # ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not resp.data or len(resp.data) == 0:
            break

        # íŠ¸ìœ„í„°ëŠ” ë³´í†µ ìµœì‹ â†’ê³¼ê±° ìˆœìœ¼ë¡œ ì „ë‹¬í•˜ë¯€ë¡œ, í˜ì´ì§€ ë‚´ì—ì„œ ë’¤ì§‘ì–´ 'ê³¼ê±°â†’ìµœì‹ ' ìˆœì„œë¡œ ì •ë ¬
        page_tweets = list(reversed(resp.data))
        pages.append((page_tweets, resp.includes))

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ì–´ì„œ, ì—†ìœ¼ë©´ ì¢…ë£Œ
        next_token = getattr(resp.meta, "next_token", None)
        if not next_token:
            break

    # ê°€ì¥ ì˜¤ë˜ëœ í˜ì´ì§€ë¶€í„°, í˜ì´ì§€ ë‚´ë¶€ë„ ì˜¤ë˜ëœ íŠ¸ìœ—ë¶€í„° ìˆœì°¨ ì²˜ë¦¬
    for page_tweets, includes in pages:
        for t in page_tweets:
            yield t, includes

def call_with_retry(func, *args, retries=5, base=1.8, **kwargs):
    """
    - 5xx(503 ë“±), ë„¤íŠ¸ì›Œí¬ ì¼ì‹œ ì˜¤ë¥˜ â†’ ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
    - 429(TooManyRequests) â†’ APIê°€ ì œê³µí•˜ëŠ” ì¬ì‹œë„ ì‹œê°„ ë˜ëŠ” ë°±ì˜¤í”„
    """
    attempt = 0
    while True:
        try:
            return func(*args, **kwargs)
        except TooManyRequests as e:
            wait = getattr(e, "retry_after", None)
            if not wait:
                wait = base ** attempt
            print(f"â³ 429 ëŒ€ê¸° {wait:.1f}s")
            time.sleep(wait)
            attempt += 1
        except TweepyHTTPException as e:
            code = getattr(getattr(e, "response", None), "status_code", None)
            if code and 500 <= code < 600 and attempt < retries:
                wait = base ** attempt
                print(f"â³ {code} ì¬ì‹œë„ {attempt+1}/{retries} (ëŒ€ê¸° {wait:.1f}s)")
                time.sleep(wait)
                attempt += 1
                continue
            raise
        except TweepyException as e:
            if attempt < retries:
                wait = base ** attempt
                print(f"â³ ì¼ì‹œ ì˜¤ë¥˜ ì¬ì‹œë„ {attempt+1}/{retries} (ëŒ€ê¸° {wait:.1f}s): {e}")
                time.sleep(wait)
                attempt += 1
                continue
            raise

def fetch_original_retweet(tweet, client, username):
    if tweet.referenced_tweets:
        for ref in tweet.referenced_tweets:
            if ref.type == "retweeted":
                response = client.get_tweet(
                    id=ref.id,
                    tweet_fields=["created_at", "text", "attachments"],
                    expansions=["attachments.media_keys"],
                    media_fields=["url", "type"]
                )

                print(response.data)
                print(response.includes)
                
                # ì›ë³¸ íŠ¸ìœ—ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (í¬ë¡¤ë§ í¬í•¨)
                original_tweet = response.data
                full_text = get_full_tweet_text(original_tweet, username)
                
                media_urls = extract_image_urls(original_tweet, response.includes)
                
                return full_text, media_urls

    # ë¦¬íŠ¸ìœ—ì´ ì•„ë‹ˆë©´
    return tweet.text, []

def get_full_tweet_text(tweet, username):
    """íŠ¸ìœ—ì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    api_text = tweet.text
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ í¬ë¡¤ë§ ì‹œë„
    if len(api_text) >= TEXT_LENGTH_THRESHOLD:
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´({len(api_text)}ì)ê°€ ì„ê³„ê°’({TEXT_LENGTH_THRESHOLD}ì)ì„ ì´ˆê³¼í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        
        # í¬ë¡¤ë§ ë¹ˆë„ ì œí•œ (ë„ˆë¬´ ìì£¼ í¬ë¡¤ë§í•˜ì§€ ì•Šë„ë¡)
        if hasattr(get_full_tweet_text, 'last_crawl_time'):
            time_since_last = time.time() - get_full_tweet_text.last_crawl_time
            if time_since_last < 30:  # 30ì´ˆ ë‚´ì— ë‹¤ì‹œ í¬ë¡¤ë§í•˜ì§€ ì•ŠìŒ
                wait_time = 30 - time_since_last + 2
                print(f"â° í¬ë¡¤ë§ ë¹ˆë„ ì œí•œ: {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ í¬ë¡¤ë§ ì§„í–‰")
                time.sleep(wait_time)
                print("âœ… ëŒ€ê¸° ì™„ë£Œ, í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # í¬ë¡¤ë§ ì „ ëœë¤ ëŒ€ê¸°
            pre_crawl_delay = random.uniform(1, 3)
            print(f"ğŸ”„ í¬ë¡¤ë§ ì „ ëŒ€ê¸°: {pre_crawl_delay:.1f}ì´ˆ")
            time.sleep(pre_crawl_delay)
            
            crawled_text = get_crawler().crawl_full_tweet_text(tweet.id, username)
            
            # í¬ë¡¤ë§ ì‹œê°„ ê¸°ë¡
            get_full_tweet_text.last_crawl_time = time.time()
            
            if crawled_text and len(crawled_text) > len(api_text):
                print(f"âœ… í¬ë¡¤ë§ìœ¼ë¡œ ë” ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤! ({len(crawled_text)}ì)")
                return crawled_text
            else:
                print(f"â„¹ï¸ í¬ë¡¤ë§ ê²°ê³¼ê°€ API í…ìŠ¤íŠ¸ì™€ ë™ì¼í•˜ê±°ë‚˜ ì§§ìŠµë‹ˆë‹¤. API í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return api_text
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return api_text
    else:
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´({len(api_text)}ì)ê°€ ì„ê³„ê°’({TEXT_LENGTH_THRESHOLD}ì) ë¯¸ë§Œì…ë‹ˆë‹¤. API í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return api_text

def extract_image_urls(tweet, includes):
    media_urls = []
    # âœ… attachmentsê°€ Noneì¸ì§€ ë¨¼ì € í™•ì¸
    if hasattr(tweet, "attachments") and tweet.attachments and "media_keys" in tweet.attachments and includes:
        media_items = includes.get("media", [])
        media_key_set = set(tweet.attachments["media_keys"])
        for media in media_items:
            if media["media_key"] in media_key_set and media["type"] == "photo":
                media_urls.append(media["url"])
    return media_urls

def send_to_telegram_with_optional_image(message: str, image_urls: List[str]):
    try:
        send_text_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

        if image_urls:
            if len(image_urls) == 1:
                # ì´ë¯¸ì§€ê°€ 1ì¥ì¼ ë•ŒëŠ” sendPhoto
                send_photo_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
                if len(message) <= MAX_CAPTION_LENGTH:
                    photo_payload = {
                        "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                        "photo": image_urls[0],
                        "caption": message
                    }
                    response = requests.post(send_photo_url, data=photo_payload)
                    response.raise_for_status()
                    print("âœ… ì‚¬ì§„+ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
                else:
                    # ë©”ì‹œì§€ê°€ ê¸¸ë©´ ì‚¬ì§„ë§Œ ë³´ë‚´ê³  í…ìŠ¤íŠ¸ ë”°ë¡œ
                    response = requests.post(send_photo_url, data={
                        "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                        "photo": image_urls[0]
                    })
                    response.raise_for_status()
                    print("âœ… ì‚¬ì§„ ì „ì†¡ ì™„ë£Œ (í…ìŠ¤íŠ¸ëŠ” ë³„ë„ ì „ì†¡)")
                    response = requests.post(send_text_url, data={
                        "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                        "text": message
                    })
                    response.raise_for_status()
                    print("âœ… í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ")
            else:
                # ì´ë¯¸ì§€ê°€ ì—¬ëŸ¬ ì¥ì¼ ë•ŒëŠ” sendMediaGroup
                send_group_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMediaGroup"
                media = []
                for i, u in enumerate(image_urls[:10]):  # ìµœëŒ€ 10ì¥
                    item = {"type": "photo", "media": u}
                    if i == 0 and len(message) <= MAX_CAPTION_LENGTH:
                        item["caption"] = message
                    media.append(item)
                response = requests.post(send_group_url, json={
                    "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                    "media": media
                })
                response.raise_for_status()
                print(f"âœ… ì‚¬ì§„ {len(media)}ì¥ ì „ì†¡ ì™„ë£Œ (ì²« ì¥ì—ë§Œ ìº¡ì…˜)")
                if len(message) > MAX_CAPTION_LENGTH:
                    # ìº¡ì…˜ ê¸¸ì´ ì´ˆê³¼ë¶„ì€ ë³„ë„ ë©”ì‹œì§€ ì „ì†¡
                    response = requests.post(send_text_url, data={
                        "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                        "text": message
                    })
                    response.raise_for_status()
                    print("âœ… ì¶”ê°€ í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ")
        else:
            # ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš°
            response = requests.post(send_text_url, data={
                "chat_id": TELEGRAM_TEST_CHANNEL_ID,
                "text": message
            })
            response.raise_for_status()
            print("âœ… í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ (ì´ë¯¸ì§€ ì—†ìŒ)")
    except Exception as e:
        print("âŒ ì „ì†¡ ì‹¤íŒ¨:", e)
        print("ğŸ“¦ ì‹¤íŒ¨í•œ ë©”ì‹œì§€:", message)

# def send_to_telegram_with_optional_image(message: str, image_urls: List[str]):
#     try:
#         send_text_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
#         if image_urls:
#             send_photo_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"

#             if len(message) <= MAX_CAPTION_LENGTH:
#                 # âœ… ì‚¬ì§„ + ë©”ì‹œì§€ í•¨ê»˜ ì „ì†¡ (caption ì‚¬ìš©)
#                 photo_payload = {
#                     "chat_id": TELEGRAM_CHANNEL_ID,
#                     "photo": image_urls[0],
#                     "caption": message
#                 }
#                 response = requests.post(send_photo_url, data=photo_payload)
#                 response.raise_for_status()
#                 print("âœ… ì‚¬ì§„+ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
#             else:
#                 # âœ… ë©”ì‹œì§€ê°€ ê¸¸ë©´ ì‚¬ì§„ë§Œ ë¨¼ì € ì „ì†¡
#                 photo_payload = {
#                     "chat_id": TELEGRAM_CHANNEL_ID,
#                     "photo": image_urls[0]
#                 }
#                 response = requests.post(send_photo_url, data=photo_payload)
#                 response.raise_for_status()
#                 print("âœ… ì‚¬ì§„ ì „ì†¡ ì™„ë£Œ (í…ìŠ¤íŠ¸ëŠ” ë³„ë„ ì „ì†¡)")

#                 # ì „ì²´ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ì „ì†¡ (ë¶„í•  ì—†ì´)
#                 text_payload = {
#                     "chat_id": TELEGRAM_CHANNEL_ID,
#                     "text": message
#                 }
#                 response = requests.post(send_text_url, data=text_payload)
#                 response.raise_for_status()
#                 print("âœ… í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ")
#         else:
#             # âœ… ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš° ì „ì²´ ë©”ì‹œì§€ë§Œ ì „ì†¡
#             text_payload = {
#                 "chat_id": TELEGRAM_CHANNEL_ID,
#                 "text": message
#             }
#             response = requests.post(send_text_url, data=text_payload)
#             response.raise_for_status()
#             print("âœ… í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ (ì´ë¯¸ì§€ ì—†ìŒ)")

#     except Exception as e:
#         print("âŒ ì „ì†¡ ì‹¤íŒ¨:", e)
#         print("ğŸ“¦ ì‹¤íŒ¨í•œ ë©”ì‹œì§€:", message)

def send_to_telegram(message):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_TEST_CHANNEL_ID,
        "text": message
    }
    response = requests.post(url, data=payload)
    response.raise_for_status()

def bootstrap_warm_start(user_id: str, username: str):
    """
    last_id íŒŒì¼ì´ ì—†ì„ ë•Œ: ìµœì‹  íŠ¸ìœ— IDë§Œ ì €ì¥í•˜ê³  ì „ì†¡ì€ ìŠ¤í‚µ.
    max_results ìµœì†Œ 5ë¡œ ë³´ì¥.
    """
    try:
        resp = call_with_retry(
            client.get_users_tweets,
            id=user_id,
            max_results=5,  # ìµœì†Œê°’ 5
            exclude=["replies", "retweets"],
            tweet_fields=["id", "created_at"]
        )
        if not resp.data:
            # íŠ¸ìœ— ìì²´ê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ 0ìœ¼ë¡œ ë§ˆí‚¹
            save_last_id(user_id, 0)
            print(f"ğŸ‘¤ @{username} warm-start: íŠ¸ìœ— ì—†ìŒ â†’ last_id=0 ì €ì¥(ìŠ¤í‚µ)")
            return

        latest_id = max(t.id for t in resp.data)
        save_last_id(user_id, latest_id)
        print(f"ğŸ‘¤ @{username} warm-start: last_id={latest_id} ì €ì¥(ì „ì†¡ ìŠ¤í‚µ)")
    except Exception as e:
        print(f"âŒ warm-start ì‹¤íŒ¨ @{username}: {e}")

# === [TRUMP RSS] ìœ í‹¸ ===
def _ts_file_path():
    return os.path.join(BASE_DIR, TRUMP_STATE_FILE)

def trump_load_last_ts() -> float:
    try:
        with open(_ts_file_path(), "r", encoding="utf-8") as f:
            return float(f.read().strip())
    except FileNotFoundError:
        return 0.0
    except Exception:
        return 0.0

def trump_save_last_ts(ts: float):
    with open(_ts_file_path(), "w", encoding="utf-8") as f:
        f.write(str(ts))

def trump_entry_ts(e) -> float:
    for k in ("published", "updated"):
        if k in e and e[k]:
            try:
                return parsedate_to_datetime(e[k]).timestamp()
            except Exception:
                pass
    return 0.0

def trump_fetch_entries_sorted():
    feed = feedparser.parse(RSS_URL)
    entries = feed.entries or []
    entries.sort(key=trump_entry_ts)  # ì˜¤ë˜ëœ â†’ ìµœì‹ 
    return entries

def trump_clean_text(html_text: str) -> str:
    text = re.sub(r"<br\s*/?>", "\n", html_text or "", flags=re.I)
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text

def trump_extract_image_urls(entry) -> List[str]:
    urls = []
    html_parts = []
    for key in ("summary", "content", "content:encoded"):
        v = entry.get(key)
        if isinstance(v, list):
            html_parts += [c.get("value", "") for c in v]
        elif v:
            html_parts.append(v)
    html_blob = "\n".join([str(x) for x in html_parts if x])
    for m in re.finditer(r'<img[^>]+src=["\']([^"\']+)["\']', html_blob, flags=re.I):
        url = m.group(1).strip()
        if url and url not in urls:
            urls.append(url)
    if "enclosures" in entry:
        for enc in entry.enclosures:
            href = enc.get("href")
            typ = (enc.get("type") or "").lower()
            if href and ("image" in typ or href.lower().endswith((".jpg",".jpeg",".png",".gif",".webp"))):
                if href not in urls:
                    urls.append(href)
    return urls

def _format_mmdd_hhmm_utc(dt: datetime) -> str:
    # X ì½”ë“œì™€ ë™ì¼í•˜ê²Œ UTC ê¸°ì¤€ìœ¼ë¡œ "%m/%d %H:%M"
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    dt_utc = dt.astimezone(timezone.utc)
    return dt_utc.strftime("%m/%d %H:%M")

def trump_format_message_like_twitter(entry_text: str, published_dt: Optional[datetime], username: str, link: str, images: List[str]) -> str:
    # ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©(ì´ëª¨ì§€/URL ë³´ì¡´)
    translated = translate_preserving_emojis_and_urls(entry_text)
    if translated is None:
        translated = "[ë²ˆì—­ ì‹¤íŒ¨: ëª¨ë“  ì—”ì§„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ]"
    created_at = _format_mmdd_hhmm_utc(published_dt or datetime.utcnow().replace(tzinfo=timezone.utc))
    # X ë©”ì‹œì§€ í¬ë§·ê³¼ ë™ì¼í•˜ê²Œ êµ¬ì„±
    msg = (
        f"ğŸ¦ ì›ë¬¸:\n{entry_text}\n\n"
        f"ğŸŒ ë²ˆì—­:\n{translated}\n\nğŸ”—"
        f"ğŸ‘¤ ì‘ì„±ì : {username}\n"
        f"ğŸ•’ ì‘ì„± ì‹œê°: {created_at}\n"
    )
    # ì›ë¬¸ ë§í¬ëŠ” ë©”ì‹œì§€ ë³¸ë¬¸ì— êµ³ì´ í•„ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ, í•„ìš”í•˜ë©´ ì•„ë˜ ì¤„ ì¶”ê°€:
    # msg += f"\nğŸ”— ì›ë¬¸: {link}\n"
    return msg

def trump_fetch_new_entries():
    entries = trump_fetch_entries_sorted()
    last_ts = trump_load_last_ts()
    new_items = [e for e in entries if trump_entry_ts(e) > last_ts]
    if entries:
        newest_ts = max(trump_entry_ts(e) for e in entries)
        trump_save_last_ts(newest_ts)
    return new_items

def trump_first_run_backfill(count: int = 3):
    entries = trump_fetch_entries_sorted()
    if not entries:
        print("âš ï¸ [TRUMP RSS] í”¼ë“œ ë¹„ì–´ìˆìŒ, ë°±í•„ ë¶ˆê°€")
        return
    backfill = entries[-count:] if count > 0 else []
    print(f"ğŸš€ [TRUMP RSS] ì²« ì‹¤í–‰ ë°±í•„: ìµœì‹  {len(backfill)}ê°œ ì „ì†¡")
    for e in backfill:
        # ë³¸ë¬¸/ë§í¬/ì‹œê°
        link = (e.get("link") or "").strip()
        body = trump_clean_text(e.get("summary") or "")
        # ì‹œê° íŒŒì‹±
        pub_raw = (e.get("published") or e.get("updated") or "").strip()
        pub_dt = None
        try:
            if pub_raw:
                pub_dt = parsedate_to_datetime(pub_raw)
        except Exception:
            pub_dt = None
        imgs = trump_extract_image_urls(e)
        msg = trump_format_message_like_twitter(body, pub_dt, TRUMP_USERNAME, link, imgs)
        send_to_telegram_with_optional_image(msg, imgs)
    newest_ts = max(trump_entry_ts(e) for e in entries)
    trump_save_last_ts(newest_ts)

def trump_poll_once():
    """íŠ¸ë£¨ìŠ¤ì†Œì…œ ìƒˆ ê¸€ì„ í•œ ë²ˆ í´ë§í•˜ì—¬ Xì™€ ë™ì¼ í¬ë§·ìœ¼ë¡œ í…”ë ˆê·¸ë¨ ì „ì†¡"""
    try:
        if trump_load_last_ts() == 0.0:
            # ì²« ì‹¤í–‰ì´ë©´ ë°±í•„ í›„ ìƒíƒœ ì €ì¥
            trump_first_run_backfill(3)
        new_entries = trump_fetch_new_entries()
        if not new_entries:
            print("ğŸ” [TRUMP RSS] ìƒˆ ê¸€ ì—†ìŒ")
            return
        for e in new_entries:
            link = (e.get("link") or "").strip()
            body = trump_clean_text(e.get("summary") or "")
            if not body.strip():
                print("âš ï¸ [TRUMP RSS] ë³¸ë¬¸ ì—†ìŒ, ìŠ¤í‚µ:", link)
                continue
            
            pub_raw = (e.get("published") or e.get("updated") or "").strip()
            pub_dt = None
            try:
                if pub_raw:
                    pub_dt = parsedate_to_datetime(pub_raw)
            except Exception:
                pub_dt = None
            imgs = trump_extract_image_urls(e)
            msg = trump_format_message_like_twitter(body, pub_dt, TRUMP_USERNAME, link, imgs)
            send_to_telegram_with_optional_image(msg, imgs)
            print("âœ… [TRUMP RSS] í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")
    except Exception as ex:
        print("âŒ [TRUMP RSS] ì²˜ë¦¬ ì˜¤ë¥˜:", ex)

def explain_tweepy_error(e):
    try:
        code = getattr(getattr(e, "response", None), "status_code", None)
        body = getattr(getattr(e, "response", None), "text", "")
        print(f"âŒ Tweepy HTTP {code}: {body}")
    except Exception:
        print(f"âŒ Tweepy error: {repr(e)}")

def run():
    print("íŠ¸ìœ— ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
    print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì„ê³„ê°’: {TEXT_LENGTH_THRESHOLD}ì")
    
    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
    last_memory_check = time.time()
    MEMORY_CHECK_INTERVAL = 1800  # 30ë¶„ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
    
    try:
        while True:
            # --- [TRUMP RSS] ë¨¼ì € í•œ ë²ˆ í´ë§ ---
            trump_poll_once()
            
            for idx, (user_id, username) in enumerate(zip(TWITTER_USER_IDS, TWITTER_USERNAMES)):
                try:
                    print(f"\nğŸš€ ì‚¬ìš©ì @{username} í™•ì¸ ì¤‘...")

                    last_id = get_last_id(user_id)

                    # ğŸ”° last_id íŒŒì¼ì´ ì—†ìœ¼ë©´: ìµœì‹  IDë§Œ ì €ì¥í•˜ê³  ì´ë²ˆ ë¼ìš´ë“œëŠ” ìŠ¤í‚µ
                    if last_id is None:
                        bootstrap_warm_start(user_id, username)
                        continue

                    max_tweet_id = last_id  # ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ë³¸ ê²ƒ ì¤‘ ê°€ì¥ í° id ì €ì¥ìš©
                    fetched_any = False

                    print(user_id)

                    # âœ… í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ since_id ì´í›„ ì „ë¶€ ê°€ì ¸ì˜¤ê¸°
                    for tweet, includes in iterate_user_tweets(user_id, last_id, page_size=100):
                        fetched_any = True

                        # ì—˜ë¡ (44196397) + quote ì œì™¸ ê·œì¹™ì´ ìˆìœ¼ë©´ ìœ ì§€
                        if user_id in EXCLUDE_QUOTE_USERS and tweet.referenced_tweets:
                            if any(ref.type == "quoted" for ref in tweet.referenced_tweets):
                                print(f"ğŸ›‘ @{username} quote íŠ¸ìœ— ì œì™¸: {tweet.id}")
                                # ë‹¤ìŒ íŠ¸ìœ—ìœ¼ë¡œ
                                if max_tweet_id is None or tweet.id > max_tweet_id:
                                    max_tweet_id = tweet.id
                                continue
                            
                        print("âœ… ìƒˆ íŠ¸ìœ— ë°œê²¬(id):", tweet.id)
                        print("âœ… ìƒˆ íŠ¸ìœ— ì‘ì„± ì‹œê°(created_at):", tweet.created_at)
                        print("âœ… ìƒˆ íŠ¸ìœ— ë°œê²¬(text):", tweet.text)

                        # ë¦¬íŠ¸ìœ—ì´ë©´ ì›ë³¸ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                        if tweet.referenced_tweets:
                            full_text, image_urls = fetch_original_retweet(tweet, client, username)
                        else:
                            full_text = get_full_tweet_text(tweet, username)
                            image_urls = extract_image_urls(tweet, includes)

                        created_at = tweet.created_at.strftime("%m/%d %H:%M")

                        # ë²ˆì—­ (None ê°€ë“œ)
                        translated_text = translate_preserving_emojis_and_urls(full_text)
                        if translated_text is None:
                            translated_text = "[ë²ˆì—­ ì‹¤íŒ¨: ëª¨ë“  ì—”ì§„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ]"

                        message = (
                            f"ğŸ¦ ì›ë¬¸:\n{full_text}\n\n"
                            f"ğŸŒ ë²ˆì—­:\n{translated_text}\n\nğŸ”—"
                            f"ğŸ‘¤ ì‘ì„±ì : {username}\n"
                            f"ğŸ•’ ì‘ì„± ì‹œê°: {created_at}\n"
                        )

                        send_to_telegram_with_optional_image(message, image_urls)
                        print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ.")

                        # ë¼ìš´ë“œ ìµœëŒ€ tweet_id ì—…ë°ì´íŠ¸
                        if max_tweet_id is None or tweet.id > max_tweet_id:
                            max_tweet_id = tweet.id

                    # ì´ë²ˆ ì‚¬ìš©ì ë¼ìš´ë“œì—ì„œ ë¬´ì–¸ê°€ ê°€ì ¸ì™”ìœ¼ë©´ last_id ê°±ì‹ 
                    if fetched_any and max_tweet_id:
                        save_last_id(user_id, max_tweet_id)
                        print(f"ğŸ‘¤ ì‘ì„±ì @{username} ğŸ“Œ max_tweet_id ì €ì¥ë¨: {max_tweet_id}")
                    else:
                        print(f"ğŸ‘¤ ì‘ì„±ì @{username} ğŸ” ìƒˆ íŠ¸ìœ— ì—†ìŒ.")

                    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ì •ë¦¬
                    current_time = time.time()
                    if current_time - last_memory_check > MEMORY_CHECK_INTERVAL:
                        monitor_memory_usage()
                        last_memory_check = current_time

                except Exception as e:
                    explain_tweepy_error(e)
                    # âœ… ì—¬ê¸°ì„œ ì¡ì•„ì£¼ë©´ 503 ë“± ì¼ì‹œ ì˜¤ë¥˜ì—ë„ í”„ë¡œì„¸ìŠ¤ê°€ ì£½ì§€ ì•ŠìŒ
                    print(f"âš ï¸ @{username} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    time.sleep(10)  # ì§§ê²Œ ì‰¬ê³  ë‹¤ìŒ ì‚¬ìš©ì/ë‹¤ìŒ ë¼ìš´ë“œ ì§„í–‰
                    continue
                    
            print("ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ : ", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            time.sleep(CHECK_INTERVAL_SECONDS)

    except KeyboardInterrupt:
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ë¨")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    finally:
        cleanup_resources()
        print("ğŸ§¹ í”„ë¡œê·¸ë¨ ì •ë¦¬ ì™„ë£Œ")

def monitor_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        print(f"ğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        if memory_mb > 500:  # 500MB ì´ìƒì´ë©´
            print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
            gc.collect()
            print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
    except Exception as e:
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")

def cleanup_resources():
    """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    global crawler
    try:
        if crawler:
            crawler.close()
            print("ğŸ§¹ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
    try:
        gc.collect()
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

def debug_single_tweet(tweet_id: str, username: str):
    """íŠ¹ì • tweet_idë§Œ ë‹¨ë°œ í…ŒìŠ¤íŠ¸"""
    try:
        resp = client.get_tweet(
            id=tweet_id,
            tweet_fields=["created_at","text","attachments","referenced_tweets"],
            expansions=["attachments.media_keys","referenced_tweets.id"],
            media_fields=["url","type"]
        )
        tweet = resp.data
        print("ğŸ” Debug Tweet")
        print("id:", tweet.id)
        print("created_at:", tweet.created_at)
        print("text:", tweet.text)
        print("referenced_tweets:", tweet.referenced_tweets)
        print("attachments:", tweet.attachments)
        print("includes:", resp.includes)

        # ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (í¬ë¡¤ë§ í¬í•¨)
        full_text = get_full_tweet_text(tweet, username)
        print("ğŸ“œ full_text:", full_text)

        # ë²ˆì—­
        translated = translate_preserving_emojis_and_urls(full_text)
        print("ğŸŒ translated:", translated)

        # ì´ë¯¸ì§€ ì¶”ì¶œ
        image_urls = extract_image_urls(tweet, resp.includes)
        print("ğŸ–¼ï¸ image_urls:", image_urls)

        # í…”ë ˆê·¸ë¨ ì „ì†¡ (í…ŒìŠ¤íŠ¸ë¼ë©´ ì£¼ì„ì²˜ë¦¬ ê°€ëŠ¥)
        created_at = tweet.created_at.strftime("%m/%d %H:%M")
        message = (
            f"ğŸ¦ ì›ë¬¸:\n{full_text}\n\n"
            f"ğŸŒ ë²ˆì—­:\n{translated}\n\n"
            f"ğŸ‘¤ ì‘ì„±ì : {username}\n"
            f"ğŸ•’ ì‘ì„± ì‹œê°: {created_at}\n"
        )
        send_to_telegram_with_optional_image(message, image_urls)
        print("âœ… í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print("âŒ debug_single_tweet ì˜¤ë¥˜:", e)


if __name__ == "__main__":
    # run() ëŒ€ì‹  ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # debug_single_tweet("1960800720061370580", "wallstengine")
    run()