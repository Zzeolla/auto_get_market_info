# stocktitan_trending_crawler.py
import os
import re
import json
import time
import html
import logging
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin
import shutil
import textwrap
from dotenv import load_dotenv

import requests
from bs4 import BeautifulSoup, Tag

TRENDING_URL = "https://www.stocktitan.net/news/trending.html"
STATE_FILE = "stocktitan_trending_state.json"  # ì§ì „ Top7 ê¸°ì–µìš©(ê¸°ì‚¬ URL ì„¸íŠ¸ ì €ì¥)

BASE = "https://www.stocktitan.net"

ARTICLE_RE = re.compile(
    r"^/news/[A-Z0-9\.\-]+/.+\.html$",  # /news/TICKER/slug.html í˜•íƒœë§Œ í—ˆìš©
)

HUB_PATHS = {
    "/news/trending.html",
    "/news/live.html",
    "/news/crypto.html",
    "/news/ai.html",
    "/news/fda-approvals.html",
    "/news/clinical-trials.html",
    "/news/today",
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_TEST_CHANNEL_ID = os.getenv("TELEGRAM_TEST_CHANNEL_ID")

RECENT_SEEN_LIMIT = 100
RECENT_EXPIRE_DAYS = 7  # 7ì¼ ë™ì•ˆë§Œ 'ì´ë¯¸ ì „ì†¡í•œ URL'ë¡œ ê°„ì£¼

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s | %(message)s"
)

def normalize_url(href: str) -> str:
    if href.startswith("http"):
        return href
    return urljoin(BASE, href)

def is_article_url(href: str) -> bool:
    """í—ˆë¸Œ/ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì œì™¸í•˜ê³ , í‹°ì»¤ ê²½ë¡œê°€ í¬í•¨ëœ ê°œë³„ ê¸°ì‚¬ë§Œ True"""
    try:
        if not href:
            return False
        if not href.startswith("http"):
            # ìƒëŒ€ê²½ë¡œë„ í—ˆìš© (ì •ê·œì‹ì€ ìƒëŒ€ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶¤)
            rel = href
        else:
            # ë„ë©”ì¸ ì™¸ ê²½ë¡œë§Œ ë¶„ë¦¬
            rel = re.sub(r"^https?://www\.stocktitan\.net", "", href)
        if rel in HUB_PATHS:
            return False
        if ARTICLE_RE.match(rel):
            return True
        return False
    except Exception:
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_state() -> Dict:
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        # í•˜ìœ„í˜¸í™˜: ì´ì „ ë²„ì „ì—” recent_urls ì—†ì„ ìˆ˜ ìˆìŒ
        data.setdefault("recent_urls", [])
        data.setdefault("last_top7_urls", [])
        return data
    except FileNotFoundError:
        return {"recent_urls": [], "last_top7_urls": []}

def _save_state(state: Dict) -> None:
    state["updated_at"] = time.time()
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def load_prev_ids() -> set:
    # ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜: ì§€ë‚œ ì‚¬ì´í´ì˜ Top7
    return set(_load_state().get("last_top7_urls", []))

def save_curr_ids(curr_ids: List[str]) -> None:
    st = _load_state()
    st["last_top7_urls"] = curr_ids
    _save_state(st)

def load_recent_seen() -> dict[str, float]:
    """ìµœê·¼ ì „ì†¡í•œ URL: timestamp ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    st = _load_state()
    recent = st.get("recent_urls", {})

    # í˜¹ì‹œ ì˜ˆì „ ë²„ì „(ë¦¬ìŠ¤íŠ¸)ì´ ë‚¨ì•„ìˆìœ¼ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if isinstance(recent, list):
        recent = {url: time.time() for url in recent}
        st["recent_urls"] = recent
        _save_state(st)
    return recent

def add_recent_seen(urls: List[str]) -> None:
    """ìƒˆë¡œ ì „ì†¡í•œ URLì„ ìµœê·¼ ëª©ë¡ì— ì¶”ê°€ (ê¸°ì¡´ ìˆìœ¼ë©´ timestampë§Œ ê°±ì‹ )."""
    st = _load_state()
    recent: dict[str, float] = st.get("recent_urls", {})
    if isinstance(recent, list):
        # ì´ì „ ë²„ì „ í˜¸í™˜ ì²˜ë¦¬
        recent = {url: time.time() for url in recent}

    now = time.time()
    for url in urls:
        recent[url] = now  # ì´ë¯¸ ìˆìœ¼ë©´ timestamp ê°±ì‹ , ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€

    # ğŸ”¹ ì˜¤ë˜ëœ í•­ëª© ì •ë¦¬ (7ì¼ ì´ìƒ ì§€ë‚œ ê²ƒì€ ìë™ ì œê±°)
    cutoff = now - RECENT_EXPIRE_DAYS * 24 * 3600
    recent = {u: ts for u, ts in recent.items() if ts >= cutoff}

    st["recent_urls"] = recent
    _save_state(st)

def get_unseen_items(data: list[dict]) -> list[dict]:
    """
    ì•„ì§ ì „ì†¡í•˜ì§€ ì•Šì€(ë˜ëŠ” ê¸°ê°„ ë§Œë£Œë¡œ ì¬ì „ì†¡ í—ˆìš©ëœ) í•­ëª©ë§Œ í•„í„°ë§.
    """
    recent = load_recent_seen()
    unseen = [d for d in data if d.get("url") not in recent]
    return unseen

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ë²ˆì—­ í›… (ì—”ì§„ ë¶™ì´ê¸° ì „ê¹Œì§€ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜)
#  - ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  Papago â†’ MS â†’ DeepL ìˆœ fallback ì—°ê²° ê¶Œì¥(ì‚¬ìš©ì ì„ í˜¸ ë°˜ì˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_with_gpt4omini(text: str, target_lang: str = "ko", source_lang: Optional[str] = None) -> str:
    if not text or text.strip() == "":
        return text
    if not OPENAI_API_KEY:
        return text  # í‚¤ ì—†ìœ¼ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ

    system_msg = (
        "You are a precise translator. Translate ONLY natural language segments into the target language. "
        "STRICTLY preserve as-is: emojis, URLs (https://...), emails, @mentions, #hashtags, $tickers, "
        "any placeholders like [EMOJI_0], {EMOJI_1}, [[EMOJI_2]], code, and original line breaks/spaces. "
        "Do not add extra text. Output only the translation."
    )
    user_msg = (f"Source language: {source_lang}\n" if source_lang else "") + \
               f"Target language: {target_lang}\n\nText:\n{text}"

    try:
        resp = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
            json={
                "model": "gpt-4o-mini",
                "temperature": 0,
                "messages": [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg},
                ],
            },
            timeout=30,
        )
        resp.raise_for_status()
        out = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        return out or text
    except Exception:
        return text  # ì‹¤íŒ¨í•˜ë©´ ì›ë¬¸ ìœ ì§€

def translate_text(text: str, target_lang: str = "ko") -> str:
    return translate_with_gpt4omini(text, target_lang=target_lang)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) íŠ¸ë Œë”© Top7 íŒŒì‹±
#    - rank/title/ticker/url + (ê°€ëŠ¥í•˜ë©´) íŠ¸ë Œë”© ì¹´ë“œì— ìˆëŠ” rhea ìš”ì•½/ê¸/ë¶€ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_trending_top7() -> List[Dict]:
    resp = requests.get(TRENDING_URL, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    items: List[Dict] = []
    seen = set()
    rank = 0

    # ìš°ì„  ì¹´ë“œ ì˜ì—­ë¶€í„° ì¢í˜€ë³´ê¸° (ìˆìœ¼ë©´ ê°€ì¥ ì •í™•)
    containers = soup.select(
        ".trending-news, #trending-news, .news-list, .cards, .container"
    )
    search_scopes = containers if containers else [soup]

    for scope in search_scopes:
        # ì¹´ë“œ ë‚´ ëŒ€í‘œ ê¸°ì‚¬ ë§í¬: â€œ/news/TICKER/slug.htmlâ€ ë§Œ
        for a in scope.select("a[href]"):
            href = a.get("href") or ""
            if not is_article_url(href):
                continue
            url = normalize_url(href)
            if url in seen:
                continue

            title = a.get_text(strip=True) or ""
            if not title:
                # ì œëª©ì´ aê°€ ì•„ë‹Œ ìƒìœ„ì— ìˆì„ ìˆ˜ë„ â†’ ì£¼ë³€ì—ì„œ ë³´ê°•
                parent = a
                for _ in range(3):
                    parent = parent.parent
                    if not parent:
                        break
                    h = parent.find(["h2", "h3"])
                    if h and h.get_text(strip=True):
                        title = h.get_text(strip=True)
                        break
            # í‹°ì»¤ ì¶”ì¶œ
            m = re.search(r"/news/([A-Z0-9\.\-]+)/", url)
            ticker = m.group(1) if m else None

            # â­ ì¹´ë“œ ë¸”ë¡ì—ì„œ ìš”ì•½/ê¸ì •/ë¶€ì •ì„ í•¨ê»˜ ì¶”ì¶œ
            card_block = a.parent
            trending_summary = extract_rhea_summary_from_block(card_block) or None
            tpos, tneg = extract_pos_neg_from_block(card_block)

            rank += 1
            items.append({
                "rank": rank,
                "title": title or "(No title)",
                "ticker": ticker,
                "url": url,
                # íŠ¸ë Œë”© ì¹´ë“œì˜ Rhea ìš”ì•½/ê¸/ë¶€ì •ì€ í•„ìš” ì‹œ ì•„ë˜ í•¨ìˆ˜ë¡œ ì‹œë„
                "trending_summary": trending_summary,
                "trending_positive": tpos,
                "trending_negative": tneg,
            })
            seen.add(url)
            if rank >= 7:
                break
        if rank >= 7:
            break

    return items[:7]

def _truncate(s: str, n: int = 400) -> str:
    # s = (s or "").strip()
    # return (s[:n-1] + "â€¦") if len(s) > n else s
    return (s or "").strip()

def _print_bullets(lines, max_items: int = 10, prefix: str = "   â€¢ "):
    if not lines:
        return
    for i, line in enumerate(lines[:max_items], 1):
        print(f"{prefix}{line}")

def _bullets(lines: list[str]) -> str:
    return "\n".join(f"   â€¢ {x}" for x in lines)

def extract_rhea_summary_from_block(block: Tag) -> Optional[str]:
    """
    íŠ¸ë Œë”© ì¹´ë“œ ë¸”ë¡ì—ì„œ 'Rhea-AI Summary' í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ ì¶”ì¶œ ì‹œë„.
    """
    if not isinstance(block, Tag):
        return None

    # í—¤ë” í…ìŠ¤íŠ¸ë¡œ êµ¬ë¶„
    candidates = []
    for lab in block.find_all(string=True):
        t = (lab or "").strip()
        if not t:
            continue
        # ë ˆì´ë¸” ì•ë’¤ë¡œ êµ¬ë¶„ë˜ëŠ” êµ¬ì¡°ë¼ë©´ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ë½‘ì•„ë³´ì
        if re.search(r"Rhea[- ]?AI Summary", t, re.I):
            # ì´ ë…¸ë“œ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ í…ìŠ¤íŠ¸ ì¶”ë¦°ë‹¤
            parent = lab.parent
            # ìš”ì•½ ë³¸ë¬¸ í›„ë³´: í˜•ì œ ìš”ì†Œ, ë‹¤ìŒ ìš”ì†Œ ë“± í­ë„“ê²Œ ì°¾ê¸°
            # ë„ˆë¬´ ê¸¸ë©´ ì ë‹¹íˆ ì˜ë¼ ì‚¬ìš©
            summary = collect_following_text(parent, stop_labels=["Positive", "Negative", "Insights"], max_chars=1800)
            if summary:
                candidates.append(summary)

    # ê°€ì¥ ê¸´ í›„ë³´ë¥¼ ì„ íƒ(ì¡ìŒ ëŒ€ë¹„)
    if candidates:
        return max(candidates, key=len)
    return None

def extract_rhea_from_detail(soup: BeautifulSoup) -> Dict:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ .article-rhea-tools ê¸°ì¤€ìœ¼ë¡œ
    summary/positive/negative/insightsë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œ.
    summaryëŠ” í•œêµ­ì–´(.summary-ko) ìš°ì„ , ì—†ìœ¼ë©´ ì˜ì–´â†’ë²ˆì—­ í›….
    """
    tools = soup.select_one(".article-rhea-tools, #article-rhea-tools") or soup  # â† ìƒì„¸ ë¸”ë¡ ì—†ì„ ë•Œë„ soupì—ì„œ íƒìƒ‰

    # --- Summary ---
    summary_ko = None
    summary_en = None
    summary_box = tools.select_one(
        ".news-card-summary.mb-2, .news-card-summary, .rhea-summary, #news-card-summary"
    )
    if summary_box:
        ko = summary_box.select_one(".summary-ko, .ko, [lang='ko'], #id-summary-ko")
        if ko and ko.get_text(strip=True):
            summary_ko = {"lang": "ko", "text": ko.get_text(" ", strip=True)}
        en = summary_box.select_one("#summary, .summary-en, .en, [lang='en'], #id-summary-en")
        if en and en.get_text(strip=True):
            summary_en = {"lang": "en", "text": en.get_text(" ", strip=True)}
        # else:
        #     en = summary_box.select_one("#summary, .summary-en, .en, [lang='en'], #id-summary-en")
        #     zh = summary_box.select_one(".summary-zh, .zh, [lang='zh'], #id-summary-zh")
        #     ja = summary_box.select_one(".summary-ja, .ja, [lang='ja'], #id-summary-ja")
        #     cand = next((el for el in [en, zh, ja] if el and el.get_text(strip=True)), None)
        #     if cand:
        #         summary = {"lang": "ko", "text": translate_text(cand.get_text(" ", strip=True), "ko")}

    # --- Positive/Negative/Insights ---
    # âœ… ìŠ¤í¬ë¦°ìƒ· êµ¬ì¡° ë°˜ì˜ (news-card-positive/negative, experts-container)
    positive_sel = (
        ".positive-points li, .rhea-positive li, .news-card-pros li, "
        "#news-card-positive li, .news-card-positive li"
    )
    negative_sel = (
        ".negative-points li, .rhea-negative li, .news-card-cons li, "
        "#news-card-negative li, .news-card-negative li"
    )
    insights_li_sel = ".insights li, .key-insights li, .takeaways li"
    insights_p_sel  = "#experts-container .accordion-body p, .insights p, .key-insights p, .takeaways p"

    positives = [li.get_text(" ", strip=True) for li in tools.select(positive_sel)]
    negatives = [li.get_text(" ", strip=True) for li in tools.select(negative_sel)]

    # InsightsëŠ” lië„ ìˆê³  pë„ ìˆì–´ ë‘˜ ë‹¤ ìˆ˜ì§‘
    insights = [li.get_text(" ", strip=True) for li in tools.select(insights_li_sel)]
    insights += [p.get_text(" ", strip=True) for p in tools.select(insights_p_sel)]

    # ì¤‘ë³µ ì œê±°
    def dedup(xs):
        out, seen = [], set()
        for x in xs:
            k = x.strip().lower()
            if k and k not in seen:
                out.append(x.strip())
                seen.add(k)
        return out

    return {
        "summary_ko": summary_ko,
        "summary_en": summary_en,
        "positive": dedup(positives),
        "negative": dedup(negatives),
        "insights": dedup(insights),
    }


def extract_pos_neg_from_block(block: Tag) -> Tuple[List[str], List[str]]:
    """
    íŠ¸ë Œë”© ì¹´ë“œì—ì„œ Positive/Negative í•­ëª©(ë¶ˆë¦¿/ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸) ì¶”ì¶œ.
    """
    positives, negatives = [], []
    if not isinstance(block, Tag):
        return positives, negatives

    sections = {
        "positive": ["positive", "positives", "pros", "bull", "bullish", "pro:"],
        "negative": ["negative", "negatives", "cons", "bear", "bearish", "con:"],
    }

    # ë¼ë²¨ í…ìŠ¤íŠ¸ ë§¤ì¹­ í›„, ì´ì–´ì§€ëŠ” bullets/ë¬¸ì¥ ìˆ˜ì§‘
    texts = block.find_all(string=True)
    for i, txt in enumerate(texts):
        t = (txt or "").strip()
        if not t:
            continue
        lower = t.lower()
        if any(key in lower for key in sections["positive"]):
            content = collect_following_text(txt.parent, stop_labels=["Negative", "Insights"], max_chars=1200)
            bullets = split_to_bullets(content)
            positives.extend(bullets)
        if any(key in lower for key in sections["negative"]):
            content = collect_following_text(txt.parent, stop_labels=["Positive", "Insights"], max_chars=1200)
            bullets = split_to_bullets(content)
            negatives.extend(bullets)

    return dedup_list(positives), dedup_list(negatives)


def collect_following_text(start_node: Tag, stop_labels: List[str], max_chars: int = 2000) -> str:
    """
    íŠ¹ì • ë ˆì´ë¸”(ì˜ˆ: 'Rhea-AI Summary')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´í›„ í˜•ì œ/ë‹¤ìŒ ë…¸ë“œë“¤ì˜
    í…ìŠ¤íŠ¸ë¥¼ ì´ì–´ ë¶™ì´ë‹¤ê°€ stop_labels ì¤‘ í•˜ë‚˜ë¥¼ ë§Œë‚˜ë©´ ë©ˆì¶¤.
    """
    if not isinstance(start_node, Tag):
        start_node = start_node.parent if hasattr(start_node, "parent") else None
    if not isinstance(start_node, Tag):
        return ""

    texts = []
    node = start_node
    char_count = 0
    for _ in range(200):  # ì•ˆì „ ì¥ì¹˜
        node = node.find_next() if hasattr(node, "find_next") else None
        if not node or not isinstance(node, Tag):
            break
        # ë©ˆì¶¤ ë ˆì´ë¸” ë„ë‹¬?
        label = node.get_text(strip=True)
        if any(lbl.lower() in label.lower() for lbl in stop_labels):
            break
        # ë§í¬/ë²„íŠ¼/ì•„ì´ì½˜ ë“±ì€ ì œì™¸
        if node.name in {"a", "button", "svg", "img", "script", "style"}:
            continue
        text = node.get_text(" ", strip=True)
        if not text:
            continue
        texts.append(text)
        char_count += len(text)
        if char_count >= max_chars:
            break

    joined = " ".join(texts)
    # ê³µë°± ì •ë¦¬
    joined = re.sub(r"\s{2,}", " ", joined).strip()
    return joined


def split_to_bullets(text: str) -> List[str]:
    if not text:
        return []
    # ì /í•˜ì´í”ˆ ëª©ë¡ ë¶„ë¦¬ ì‹œë„
    parts = re.split(r"(?:\n|\r|â€¢|-|\u2022)\s*", text)
    parts = [p.strip(" -â€¢\t\r\n") for p in parts if p and len(p.strip()) > 1]
    return parts[:8]  # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ ëª‡ ê°œë§Œ


def dedup_list(xs: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in xs:
        k = x.lower()
        if k not in seen:
            out.append(x)
            seen.add(k)
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ê¸°ì‚¬ ìƒì„¸ íŒŒì‹±
#    - ìƒì„¸ì— Rhea-AI Summary/Positive/Negative/Insightsê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
#    - ì—†ìœ¼ë©´ íŠ¸ë Œë”© ì¹´ë“œì—ì„œ ê°€ì ¸ì˜¨ ê°’ìœ¼ë¡œ fallback
#    - ê¸°ì‚¬ ë³¸ë¬¸ì„ ì„¹ì…˜/í—¤ë”(êµµê²Œ/ì œëª©) í¬í•¨í•˜ì—¬ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_article_detail(url: str) -> Dict:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # ë©”íƒ€
    title = (soup.select_one("h1") or soup.select_one("title"))
    title_text = title.get_text(strip=True) if title else ""

    published_at = extract_published_at(soup)
    source_url = extract_source_url(soup)

    # âœ… ìƒì„¸ í˜ì´ì§€ì˜ Rhea-AI ë¸”ë¡(êµ¬ì¡° ê¸°ë°˜) íŒŒì‹±
    rhea = extract_rhea_from_detail(soup)

    # ê¸°ì‚¬ ë³¸ë¬¸(í—¤ë”/ë³¼ë“œ ê°ì§€ í¬í•¨) â€” ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€
    body_sections = extract_article_body_sections(soup)

    return {
        "title": title_text,
        "published_at": published_at,
        "source_url": source_url,
        "detail": {
            "summary_ko": rhea["summary_ko"],     # {"lang":"ko","text":"..."} or None
            "summary_en": rhea["summary_en"],
            "positive": rhea["positive"],    # list[str]
            "negative": rhea["negative"],    # list[str]
            "insights": rhea["insights"],    # list[str]
        },
        "body": body_sections,
    }


def extract_published_at(soup: BeautifulSoup) -> Optional[str]:
    # ë‚ ì§œ í¬ë§·ì´ ê¸°ì‚¬ë§ˆë‹¤ ë‹¬ë¼ì„œ ì—¬ëŸ¬ í›„ë³´ë¥¼ íƒìƒ‰
    # common patterns: time[datetime], meta[property='article:published_time'], 'Published' í…ìŠ¤íŠ¸ ê·¼ì²˜ ë“±
    meta = soup.select_one("meta[property='article:published_time'], meta[name='pubdate'], time[datetime]")
    if meta:
        dt = meta.get("content") or meta.get("datetime")
        if dt:
            return dt
    # ë°±ì—…: í˜ì´ì§€ ë‚´ í…ìŠ¤íŠ¸ ìŠ¤ìº”
    for el in soup.find_all(string=True):
        t = (el or "").strip()
        if re.search(r"\b(Published|Updated)\b", t, re.I) and len(t) < 120:
            return t
    return None


def extract_source_url(soup: BeautifulSoup) -> Optional[str]:
    # â€œView source version on â€¦â€ ë§í¬ë¥¼ ì°¾ê¸°
    for a in soup.select("a[href]"):
        label = a.get_text(" ", strip=True).lower()
        if "view source" in label or "source version" in label:
            href = a.get("href")
            if href and href.startswith("http"):
                return href
    return None

def collect_multilang_summary(container: Tag) -> Dict[str, str]:
    """
    í•œ ì»¨í…Œì´ë„ˆ ë‚´ ë‹¤êµ­ì–´ ìš”ì•½ì´ ìˆëŠ” ê²½ìš°(ì˜ˆ: ì˜ì–´/í•œêµ­ì–´/ì¤‘êµ­ì–´ íƒ­),
    ì–¸ì–´ ë¼ë²¨ì„ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ê°ì§€í•´ {lang_code: text} ë¡œ ë°˜í™˜.
    """
    text_map: Dict[str, str] = {}
    # í”í•œ ì–¸ì–´ ë¼ë²¨ í…ìŠ¤íŠ¸
    lang_aliases = {
        "ko": ["korean", "í•œêµ­ì–´", "ko"],
        "en": ["english", "ì˜ì–´", "en"],
        "zh": ["chinese", "ä¸­æ–‡", "zh"],
        "ja": ["japanese", "æ—¥æœ¬èª", "ja"],
        "es": ["spanish", "espaÃ±ol", "es"],
    }

    # ì»¨í…Œì´ë„ˆ ì¸ê·¼ì—ì„œ ì–¸ì–´ ë¼ë²¨ + ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ìˆ˜ì§‘
    # êµ¬ì¡°ê°€ ì œê°ê°ì´ë¼, ë¼ë²¨ í›„ë³´ë¥¼ ë¨¼ì € ì°¾ê³  ë’¤ë”°ë¥´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¬¶ëŠ”ë‹¤
    labels = container.find_all(string=True)
    for i, raw in enumerate(labels):
        t = (raw or "").strip()
        if not t:
            continue
        lower = t.lower()
        for lang, keys in lang_aliases.items():
            if any(k in lower for k in keys):
                # ì´ ë¼ë²¨ ì´í›„ í…ìŠ¤íŠ¸ ë¸”ë¡ ëª¨ìœ¼ê¸°
                txt = collect_following_text(raw.parent, stop_labels=list(sum(lang_aliases.values(), [])), max_chars=1800)
                if txt:
                    text_map[lang] = txt
    # ì–¸ì–´ ë¼ë²¨ì´ ì•ˆ ë³´ì´ë©´, ì»¨í…Œì´ë„ˆ ìì²´ í…ìŠ¤íŠ¸ë¥¼ í†µìœ¼ë¡œ ì·¨í•¨(ì˜ì–´ ê°€ì •)
    if not text_map:
        bulk = container.get_text(" ", strip=True)
        bulk = re.sub(r"\s{2,}", " ", bulk).strip()
        if bulk:
            text_map["en"] = bulk
    return text_map

def extract_article_body_sections(soup: BeautifulSoup) -> List[Dict]:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì„ 'í—¤ë”/ë¬¸ë‹¨' ë‹¨ìœ„ë¡œ ì¶”ì¶œ.
    - í—¤ë”: h1~h4, strong/b(ë³¼ë“œ) â†’ type='header', level=1~4 ë˜ëŠ” 5
    - ë¬¸ë‹¨/ë¦¬ìŠ¤íŠ¸: type='paragraph' / 'list_item'
    """
    container = find_main_article_container(soup) or soup

    sections: List[Dict] = []

    # ì œëª© ê³„ì¸µ
    for el in container.find_all(["h1", "h2", "h3", "h4", "p", "li", "strong", "b"]):
        name = el.name.lower()
        text = el.get_text(" ", strip=True)
        if not text or len(text) < 2:
            continue

        if name in {"h1", "h2", "h3", "h4"}:
            level = int(name[1])
            sections.append({"type": "header", "level": level, "text": text})
        elif name in {"strong", "b"}:
            # ê°•í•œ ê°•ì¡°ë¥¼ 5ë ˆë²¨ í—¤ë”ì²˜ëŸ¼ ì·¨ê¸‰ (ì¤‘ë³µ ë°©ì§€)
            if not ends_with_punctuation(text):
                sections.append({"type": "header", "level": 5, "text": text})
        elif name == "li":
            sections.append({"type": "list_item", "text": text})
        else:
            sections.append({"type": "paragraph", "text": text})

    # ì¸ì ‘í•œ ì¤‘ë³µ/ë…¸ì´ì¦ˆ ì •ë¦¬
    cleaned: List[Dict] = []
    prev = None
    for s in sections:
        if prev and s == prev:
            continue
        cleaned.append(s)
        prev = s

    return cleaned


def find_main_article_container(soup: BeautifulSoup) -> Optional[Tag]:
    # í”í•œ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í›„ë³´ë“¤
    selectors = [
        "article", ".article", ".post", ".news", "#content", ".content"
    ]
    for sel in selectors:
        node = soup.select_one(sel)
        if node:
            return node
    # ëª» ì°¾ìœ¼ë©´ None
    return None


def ends_with_punctuation(text: str) -> bool:
    return bool(re.search(r"[.!?â€¦]$", text))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ í”Œë¡œìš°(ìƒ˜í”Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_once() -> List[Dict]:
    trending = fetch_trending_top7()
    curr_ids = [item["url"] for item in trending]
    prev_ids = load_prev_ids()

    new_ids = set(curr_ids) - prev_ids
    logging.info(f"Top7 total: {len(curr_ids)}, new_in_rank: {len(new_ids)}")

    results = []
    for item in trending:
        url = item["url"]
        detail = parse_article_detail(url)

        # Rhea-AI ìš°ì„ ìˆœìœ„: ìƒì„¸ â†’ íŠ¸ë Œë”© fallback
        summary_ko = detail["detail"]["summary_ko"]
        summary_en = detail["detail"]["summary_en"]
        # if not summary_ko and item.get("trending_summary"):
        #     summary_ko = {"lang": "ko", "text": translate_text(item["trending_summary"], "ko")}

        positives = detail["detail"]["positive"] or []
        # if not positives and item.get("trending_positive"):
        #     positives = [translate_text(x, "ko") for x in item["trending_positive"]]

        negatives = detail["detail"]["negative"] or []
        # if not negatives and item.get("trending_negative"):
        #     negatives = [translate_text(x, "ko") for x in item["trending_negative"]]

        insights = detail["detail"]["insights"] or []

        def _tko(s: Optional[str]) -> str:
            return translate_text(s, "ko") if s else ""
        
        if not summary_ko and summary_en and summary_en.get("text"):
            summary_ko = {"lang": "ko", "text": _tko(summary_en["text"])}
        
        positives_ko = [_tko(x) for x in positives] if positives else []
        negatives_ko = [_tko(x) for x in negatives] if negatives else []
        insights_ko = [_tko(x) for x in insights] if insights else []

        results.append({
            "rank": item["rank"],
            "ticker": item["ticker"],
            "title": detail["title"] or item["title"],
            "url": url,
            "published_at": detail["published_at"],
            "source_url": detail["source_url"],
            # ì›ë¬¸/ì˜ë¬¸ ê¸°ë°˜ ë¸”ë¡
            "rhea_ai": {
                "summary": summary_en,          # {"lang":"en","text":...} or None
                "positive": positives,          # list[str] (ì˜ë¬¸/ì›ë¬¸)
                "negative": negatives,          # list[str]
                "insights": insights            # list[str]
            },

            # í•œêµ­ì–´ ë¸”ë¡ (ìš”ì•½ + ë¶ˆë¦¿ ì „ë¶€ ë²ˆì—­/ë³´ì™„)
            "rhea_ai_ko": {
                "summary":  summary_ko,         # {"lang":"ko","text":...} or None
                "positive": positives_ko,       # list[str] (ko)
                "negative": negatives_ko,       # list[str] (ko)
                "insights": insights_ko         # list[str] (ko)
            },

            "body": detail["body"],               # ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
            "is_new_in_rank": url in new_ids,     # ì´ë²ˆ ì£¼ê¸°ì—ì„œ ìƒˆë¡œ ì§„ì…í–ˆëŠ”ê°€?
            "captured_at": datetime.now(timezone.utc).isoformat()
        })

    # ë§ˆì§€ë§‰ì— Top7 URL ì„¸íŠ¸ ê°±ì‹ 
    save_curr_ids(curr_ids)
    return results

def build_tg_message(d: Dict) -> str:
    lines = []
    ticker = d.get("ticker") or "-"
    title = (d.get("title") or "").strip()

    lines.append(f"ğŸ†• [{ticker}] {title}")

def send_to_telegram(message: str):
    try:
        send_text_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

        response = requests.post(send_text_url, data={
            "chat_id": TELEGRAM_TEST_CHANNEL_ID,
            "text": message
        })
        response.raise_for_status()
        print("âœ… í…ìŠ¤íŠ¸ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print("âŒ ì „ì†¡ ì‹¤íŒ¨:", e)
        print("ğŸ“¦ ì‹¤íŒ¨í•œ ë©”ì‹œì§€:", message)

if __name__ == "__main__":

    while True:
        try:
            data = run_once()

            new_items = get_unseen_items(data)

            # ğŸ”‡ ìƒˆ ì§„ì…ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ì•Šê³  ë‹¤ìŒ ì‚¬ì´í´ë¡œ
            if not new_items:
                # time.sleep(600) ì „ì— ë°”ë¡œ ë„˜ì–´ê° (ì¶œë ¥ ì—†ìŒ)
                time.sleep(600)
                continue

            sent_urls_batch = []  # ì´ë²ˆ ì‚¬ì´í´ì— ì‹¤ì œ ì „ì†¡ëœ URL ëˆ„ì 

            # ì½˜ì†” ì¶œë ¥(ìš”ì•½)
            for d in new_items:

                ticker = d.get("ticker") or "-"
                title = (d.get("title") or "").strip()
                rs = (d.get("rhea_ai") or {}).get("summary") or {}
                summary_text = _truncate(rs.get("text", ""))
                positives = (d.get("rhea_ai") or {}).get("positive") or []
                negatives = (d.get("rhea_ai") or {}).get("negative") or []
                insights = (d.get("rhea_ai") or {}).get("insights") or []

                msg = (
                    f"ğŸ¦ ì›ë¬¸\n\n"
                    f"ğŸ†• [{ticker}] {title}\n"
                    f"ğŸ“ Summary\n"
                    f"   {summary_text}"
                    f"\nğŸŸ¢ Positive\n"
                    f"{_bullets(positives)}"
                    f"\nğŸ”´ Negative\n"
                    f"{_bullets(negatives)}"
                    f"\nğŸ’¡ Insights\n"
                    f"{_bullets(insights)}"
                )

                title_ko = translate_text(title, "ko")
                rs_ko = (d.get("rhea_ai_ko") or {}).get("summary") or {}
                summary_text_ko = _truncate(rs_ko.get("text", ""))
                positives_ko = (d.get("rhea_ai_ko") or {}).get("positive") or []
                negatives_ko = (d.get("rhea_ai_ko") or {}).get("negative") or []
                insights_ko = (d.get("rhea_ai_ko") or {}).get("insights") or []

                msg_ko = (
                    f"ğŸŒ ë²ˆì—­\n\n"
                    f"ğŸ†• [{ticker}] {title_ko}\n"
                    f"ğŸ“ Summary\n"
                    f"   {summary_text_ko}"
                    f"\nğŸŸ¢ Positive\n"
                    f"{_bullets(positives_ko)}"
                    f"\nğŸ”´ Negative\n"
                    f"{_bullets(negatives_ko)}"
                    f"\nğŸ’¡ Insights\n"
                    f"{_bullets(insights_ko)}"
                    f"\n\nğŸ”— URL\n"
                    f"{d['url']}"
                )

                # combined = f"{msg}\n\n{'â”€'*24}\n\n{msg_ko}"

                send_to_telegram(msg_ko)

                #send_to_telegram(msg)

                # â‘¢ ì „ì†¡ ì„±ê³µí•œ URLì„ ë°°ì¹˜ì— ëª¨ì•„ë‘ 
                sent_urls_batch.append(d["url"])

            # â‘£ í•œ ë²ˆì— recent_urls ì—…ë°ì´íŠ¸(ì¤‘ë³µ ë°©ì§€, ìµœëŒ€ 100)
            if sent_urls_batch:
                add_recent_seen(sent_urls_batch)
                
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘  ì œëª©
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # print(f"ğŸ†• [{ticker}] {title}")

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘¡ Summary
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # if rs and rs.get("text"):
                #     print("ğŸ“ Summary:")
                #     print("   " + _truncate(rs["text"]))

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘¢ Positive (ğŸŸ¢)
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # if positives:
                #     print(f"ğŸŸ¢ Positive:")
                #     _print_bullets(positives)

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘£ Negative (ğŸ”´)
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # if negatives:
                #     print(f"ğŸ”´ Negative:")
                #     _print_bullets(negatives)

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘¤ Insights (ğŸ’¡)
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # if insights:
                #     print(f"ğŸ’¡ Insights:")
                #     _print_bullets(insights)

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # â‘¥ Links
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # print()
                # if d.get("url"):
                #     print(f"ğŸ”— {d['url']}")

                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # # ê¸°ì‚¬ ê°„ êµ¬ë¶„ì„ 
                # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # print()

        except KeyboardInterrupt:
            print("\nâ¹ï¸ Stopped by user.")
            break
        except Exception as e:
            logging.exception("cycle error")   # ì „ì²´ ìŠ¤íƒ ì¶œë ¥
            # ì—ëŸ¬ ì‹œë„ ì¡°ìš©íˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ (ì›í•˜ë©´ ë¡œê·¸ë¡œ ë°”ê¿”ë„ ë¨)
            # print(f"[WARN] cycle error: {e}")
            time.sleep(600)
            continue

        # ë‹¤ìŒ ì‚¬ì´í´ê¹Œì§€ 10ë¶„ ëŒ€ê¸°
        time.sleep(600)